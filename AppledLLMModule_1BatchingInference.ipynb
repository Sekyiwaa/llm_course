{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sekyiwaa/llm_course/blob/main/AppledLLMModule_1BatchingInference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxndKsyOoKh1",
        "outputId": "a557937c-f316-4001-be14-b51ce548bd00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Generation till Completion and Batch Processing\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation till completion\n",
        "# so now let's talk about elephant in the room\n",
        "# llm are talkative, and since we are predicting the next token\n",
        "# we need to account for the idea that\n",
        "# 1) llm may finish it's logic or generation of text # with special token <eos>\n",
        "# 2) we don't care and after specified number of new tokens generated we halt\n",
        "# the generation, rude not user friendly but necessity\n",
        "\n",
        "print(tokenizer.eos_token)\n",
        "print(tokenizer.eos_token_id)\n",
        "\n",
        "# this should be your first command running after initialization\n",
        "# so basically we want to keep generating new tokens like before\n",
        "# and stop generating upon reaching condition 1 or 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbExDL9OvmoH",
        "outputId": "46c91e8b-e5f7-41a1-954d-d8399d646221"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|endoftext|>\n",
            "151643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model,tokenizer,prompt,max_new_tokens=15):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    generated_tokens = list()\n",
        "\n",
        "    for step in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask)\n",
        "\n",
        "        logits_last = out.logits[:,-1,:]\n",
        "        probs = F.softmax(logits_last,dim=-1)\n",
        "\n",
        "        # so what we do is called greedy decoding, we will pick up the token\n",
        "        # with highest probability\n",
        "        next_token_id = torch.argmax(probs,dim=-1)\n",
        "\n",
        "        token_id_int = next_token_id.item()\n",
        "\n",
        "        # next token text\n",
        "        next_token = tokenizer.decode(token_id_int)\n",
        "\n",
        "\n",
        "        # ok and now pay attention at this moment your next token may say eos\n",
        "        # or effectively saying I am done predicting next token\n",
        "        # most important line\n",
        "        if tokenizer.eos_token_id is not None and token_id_int == tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # now let's update inputs, since we want to continue generations\n",
        "        # with newly minted token in context\n",
        "\n",
        "        # append to context: make shape [1,1], then cat\n",
        "        next_token_2d = next_token_id.view(1, 1)        # [1, 1]\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_2d], dim=1)\n",
        "\n",
        "        attention_mask = torch.cat(\n",
        "            [attention_mask, torch.ones((1, 1), dtype=attention_mask.dtype)], dim=1\n",
        "        )\n",
        "\n",
        "    continuation = \"\".join(generated_tokens)\n",
        "    return continuation, prompt + continuation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qL1leJ_sxK3V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Capital of Texas is \"\n",
        "new_text,full = generate(model,tokenizer,prompt,25)\n"
      ],
      "metadata": {
        "id": "4-yR-YiY3pg9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(new_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h40MZyqP4Wh-",
        "outputId": "8b84ac93-4f8b-487e-81cc-13a1617da846"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 miles from Austin, Texas. How far is it from Austin, Texas to the capital of Texas?\n",
            "To determine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# last time you used pipeline\n",
        "# well now use torch to generate input to help the user understand how next token is generated\n",
        "# for each token shsow the user other 5 top tokens you skipped or didn't show, not just the best token\n",
        "# for example input = hello\n",
        "# output = hello 1st predicted token: world [2nd:john, 3rd:welcome, 4th:good, 5th:buy]\n",
        "\n",
        "# Example Input = How are you doing?\n",
        "# Output = 1st predicted token: Hi [2nd: 3rd: 4th: 5th: ]\n",
        "# Output = 2nd predicted token: I'm [2nd: 3rd: 4th: 5th: ]\n",
        "# Output = 3rd predicted token: Doing [2nd: 3rd: 4th: 5th: ]\n",
        "# Output = 4th predicted token: Great [2nd: 3rd: 4th: 5th: ]\n",
        "# Output = 5th predicted token: ! [2nd: 3rd: 4th: 5th: ]\n",
        "# Output = 5th predicted token: <eos> [2nd: 3rd: 4th: 5th: ]\n"
      ],
      "metadata": {
        "id": "2rcFXPn26Qdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ClZsLLC5LQK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143f6148",
        "outputId": "ba019d78-4ce3-4e7a-dc43-232acfe156a8"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def generate(model, tokenizer, prompt): # Removed max_new_tokens parameter\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    generated_tokens = list()\n",
        "\n",
        "    print(f\"Input: {prompt}\")\n",
        "\n",
        "    # Continue generation until the eos token is generated\n",
        "    while True:\n",
        "        with torch.no_grad():\n",
        "            out = model(input_ids = input_ids,\n",
        "                        attention_mask = attention_mask)\n",
        "\n",
        "        logits_last = out.logits[:,-1,:]\n",
        "        probs = F.softmax(logits_last,dim=-1)\n",
        "\n",
        "        # Get top 5 tokens and their probabilities\n",
        "        top_k_probs, top_k_indices = torch.topk(probs, k=5)\n",
        "\n",
        "        # so what we do is called greedy decoding, we will pick up the token\n",
        "        # with highest probability\n",
        "        next_token_id_tensor = top_k_indices[0, 0]\n",
        "        token_id_int = next_token_id_tensor.item()\n",
        "\n",
        "        # next token text\n",
        "        next_token = tokenizer.decode(token_id_int)\n",
        "\n",
        "        # Decode top 5 tokens and get their probabilities\n",
        "        top_5_tokens_with_probs = [(tokenizer.decode(idx.item()), prob.item()) for idx, prob in zip(top_k_indices[0], top_k_probs[0])]\n",
        "\n",
        "\n",
        "        print(f\"Step {len(generated_tokens)+1}: Predicted Token: {next_token} [Top 5: {top_5_tokens_with_probs}]\")\n",
        "\n",
        "\n",
        "        # ok and now pay attention at this moment your next token may say eos\n",
        "        # or effectively saying I am done predicting next token\n",
        "        # most important line\n",
        "        if tokenizer.eos_token_id is not None and token_id_int == tokenizer.eos_token_id:\n",
        "            print(\"<eos> token generated, stopping.\")\n",
        "            break\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "        # now let's update inputs, since we want to continue generations\n",
        "        # with newly minted token in context\n",
        "\n",
        "        # append to context: make shape [1,1], then cat\n",
        "        next_token_2d = next_token_id_tensor.view(1, 1)        # [1, 1]\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_2d], dim=1)\n",
        "\n",
        "        attention_mask = torch.cat(\n",
        "            [attention_mask, torch.ones((1, 1), dtype=attention_mask.dtype)], dim=1\n",
        "        )\n",
        "\n",
        "    continuation = \"\".join(generated_tokens)\n",
        "    return continuation, prompt + continuation\n",
        "\n",
        "prompt = \"Capital of Texas is \"\n",
        "new_text,full = generate(model,tokenizer,prompt) # Removed max_new_tokens argument\n",
        "print(\"\\nGenerated Text:\", new_text)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: Capital of Texas is \n",
            "Step 1: Predicted Token: 1 [Top 5: [('1', 0.23321795463562012), ('2', 0.15147124230861664), ('3', 0.10503458976745605), ('5', 0.08549241721630096), ('6', 0.08106186240911484)]]\n",
            "Step 2: Predicted Token: 0 [Top 5: [('0', 0.20056715607643127), ('2', 0.08730699867010117), ('1', 0.08246234804391861), ('5', 0.07479152828454971), ('6', 0.06720951944589615)]]\n",
            "Step 3: Predicted Token: 0 [Top 5: [('0', 0.3742975890636444), (' miles', 0.10339178144931793), (',', 0.055691592395305634), ('5', 0.050429508090019226), ('2', 0.04000747948884964)]]\n",
            "Step 4: Predicted Token:  miles [Top 5: [(' miles', 0.3537689447402954), ('0', 0.15094760060310364), (' km', 0.05416981503367424), (' years', 0.045800428837537766), (',', 0.04548215493559837)]]\n",
            "Step 5: Predicted Token:  from [Top 5: [(' from', 0.21614880859851837), (' north', 0.13268116116523743), (' west', 0.13078369200229645), (' east', 0.1178579181432724), (' away', 0.06823877990245819)]]\n",
            "Step 6: Predicted Token:  Austin [Top 5: [(' Austin', 0.20885799825191498), (' the', 0.13216209411621094), (' San', 0.05906642600893974), (' Dallas', 0.05532123148441315), (' Houston', 0.03436867147684097)]]\n",
            "Step 7: Predicted Token: , [Top 5: [(',', 0.3901772201061249), ('.', 0.3045218884944916), (' and', 0.10379824787378311), (' to', 0.06821317970752716), ('.\\n', 0.028073234483599663)]]\n",
            "Step 8: Predicted Token:  Texas [Top 5: [(' Texas', 0.21603231132030487), (' ', 0.20860320329666138), (' which', 0.18986038863658905), (' and', 0.12022105604410172), (' TX', 0.09392230212688446)]]\n",
            "Step 9: Predicted Token: . [Top 5: [('.', 0.4390757381916046), (' is', 0.19179382920265198), (',', 0.16502565145492554), (' and', 0.042909663170576096), (' ', 0.01914660818874836)]]\n",
            "Step 10: Predicted Token:  How [Top 5: [(' How', 0.1536657214164734), (' The', 0.14038322865962982), (' If', 0.13239488005638123), (' Austin', 0.09588892012834549), (' It', 0.042571764439344406)]]\n",
            "Step 11: Predicted Token:  far [Top 5: [(' far', 0.6151822805404663), (' many', 0.2905627191066742), (' long', 0.06594377756118774), (' much', 0.015328285284340382), (' close', 0.002425536746159196)]]\n",
            "Step 12: Predicted Token:  is [Top 5: [(' is', 0.4850783944129944), (' away', 0.2263595163822174), (' apart', 0.14016138017177582), (' will', 0.029662132263183594), (' would', 0.02658911421895027)]]\n",
            "Step 13: Predicted Token:  it [Top 5: [(' it', 0.7000610828399658), (' the', 0.131887286901474), (' Austin', 0.03245392069220543), (' a', 0.0053684706799685955), (' that', 0.005209238268435001)]]\n",
            "Step 14: Predicted Token:  from [Top 5: [(' from', 0.8487130999565125), (' to', 0.04906962066888809), (' between', 0.0372990258038044), (' in', 0.02259557694196701), (',', 0.010980995371937752)]]\n",
            "Step 15: Predicted Token:  Austin [Top 5: [(' Austin', 0.5819679498672485), (' the', 0.15804427862167358), (' Houston', 0.038120973855257034), (' Dallas', 0.030831966549158096), (' San', 0.01130794920027256)]]\n",
            "Step 16: Predicted Token: , [Top 5: [(',', 0.6554660797119141), (' to', 0.31573179364204407), (' Texas', 0.008020888082683086), (' and', 0.0025334241800010204), (\"'s\", 0.0016963183879852295)]]\n",
            "Step 17: Predicted Token:  Texas [Top 5: [(' Texas', 0.9771586656570435), (' TX', 0.007102400064468384), (' New', 0.001893482985906303), (' to', 0.0016977231716737151), (' the', 0.0010006790980696678)]]\n",
            "Step 18: Predicted Token:  to [Top 5: [(' to', 0.9384370446205139), (',', 0.013387668877840042), (' by', 0.007679786533117294), (' and', 0.0038159508258104324), (' if', 0.0035991277545690536)]]\n",
            "Step 19: Predicted Token:  the [Top 5: [(' the', 0.38297194242477417), (' Dallas', 0.03258417174220085), (' Corpus', 0.028667835518717766), (' San', 0.02526865154504776), (' Austin', 0.02062939666211605)]]\n",
            "Step 20: Predicted Token:  capital [Top 5: [(' capital', 0.24083271622657776), (' Capital', 0.20558319985866547), (' nearest', 0.12030289322137833), (' city', 0.047502439469099045), (' border', 0.04187839850783348)]]\n",
            "Step 21: Predicted Token:  of [Top 5: [(' of', 0.7400023341178894), (' city', 0.14720037579536438), (',', 0.03408516198396683), ('?\\n', 0.025293296203017235), ('?', 0.019087249413132668)]]\n",
            "Step 22: Predicted Token:  Texas [Top 5: [(' Texas', 0.7842379808425903), (' New', 0.0342833548784256), (' Oklahoma', 0.021761298179626465), (' the', 0.018001891672611237), (' Louisiana', 0.008404029533267021)]]\n",
            "Step 23: Predicted Token: ?\n",
            " [Top 5: [('?\\n', 0.450029581785202), ('?', 0.3004821538925171), (',', 0.0782596692442894), ('?\\n\\n', 0.06291183829307556), (' in', 0.01186512690037489)]]\n",
            "Step 24: Predicted Token: To [Top 5: [('To', 0.319131076335907), ('Answer', 0.1962636411190033), ('The', 0.13000793755054474), ('If', 0.11936958879232407), ('A', 0.06074689328670502)]]\n",
            "Step 25: Predicted Token:  determine [Top 5: [(' determine', 0.6221722960472107), (' find', 0.27459684014320374), (' solve', 0.051011040806770325), (' calculate', 0.03206982463598251), (' answer', 0.011178635060787201)]]\n",
            "Step 26: Predicted Token:  the [Top 5: [(' the', 0.9458670020103455), (' how', 0.054095908999443054), (' if', 1.0757167729025241e-05), (' from', 7.849468602216803e-06), (' that', 1.8369589724898105e-06)]]\n",
            "Step 27: Predicted Token:  distance [Top 5: [(' distance', 0.9739056825637817), (' total', 0.01218357589095831), (' straight', 0.009001684375107288), (' direct', 0.0026906104758381844), (' distances', 0.000295308418571949)]]\n",
            "Step 28: Predicted Token:  from [Top 5: [(' from', 0.6887947916984558), (' between', 0.30877044796943665), (' of', 0.0010116978082805872), (' in', 0.0003626297111622989), (' it', 0.00035701721208170056)]]\n",
            "Step 29: Predicted Token:  Austin [Top 5: [(' Austin', 0.9934750199317932), (' the', 0.003857732517644763), (' Texas', 0.0003754451754502952), (' Houston', 0.00023008621064946055), (' Dallas', 0.00017235390259884298)]]\n",
            "Step 30: Predicted Token: , [Top 5: [(',', 0.9478799700737), (' to', 0.050982244312763214), (' (', 0.00043678050860762596), (' in', 0.0003212966548744589), (' Texas', 0.00012175870506325737)]]\n",
            "Step 31: Predicted Token:  Texas [Top 5: [(' Texas', 0.9979433417320251), (' which', 0.0007720208959653974), (' TX', 0.0006355984369292855), (' the', 0.00035933955223299563), (' where', 3.614297020249069e-05)]]\n",
            "Step 32: Predicted Token: , [Top 5: [(',', 0.5488204956054688), (' to', 0.4395117461681366), (' (', 0.010558581911027431), (' and', 0.0002528534969314933), (' at', 0.00013194704661145806)]]\n",
            "Step 33: Predicted Token:  to [Top 5: [(' to', 0.9185742139816284), (' which', 0.07352858781814575), (' where', 0.0023127386812120676), (' located', 0.0009223404340445995), (' ', 0.0008120939019136131)]]\n",
            "Step 34: Predicted Token:  the [Top 5: [(' the', 0.9400020241737366), (' its', 0.05844322219491005), (' Austin', 0.000616415694821626), (' Dallas', 0.0001229726622113958), (' Houston', 7.39184397389181e-05)]]\n",
            "Step 35: Predicted Token:  capital [Top 5: [(' capital', 0.9916673302650452), (' Capital', 0.003342148382216692), (' cap', 0.0020897684153169394), (' city', 0.001550758141092956), (' state', 0.0002494049840606749)]]\n",
            "Step 36: Predicted Token:  of [Top 5: [(' of', 0.989510178565979), (',', 0.0047741965390741825), (' city', 0.004657309036701918), (' (', 0.0004785955243278295), (' in', 0.00010489502892596647)]]\n",
            "Step 37: Predicted Token:  Texas [Top 5: [(' Texas', 0.9994722008705139), (' the', 0.0003698884684126824), (' New', 1.7070729882107116e-05), (' Austin', 1.221505954163149e-05), ('Texas', 1.0476290299266111e-05)]]\n",
            "Step 38: Predicted Token: , [Top 5: [(',', 0.9597977995872498), (' (', 0.03534562513232231), (' in', 0.0010987864807248116), (' which', 0.0005683980998583138), (' using', 0.0005167413037270308)]]\n",
            "Step 39: Predicted Token:  we [Top 5: [(' we', 0.8631478548049927), (' which', 0.08968895673751831), (' let', 0.01853213645517826), (' ', 0.011272943578660488), (' you', 0.0027239162009209394)]]\n",
            "Step 40: Predicted Token:  need [Top 5: [(' need', 0.6662169694900513), (' can', 0.16177435219287872), (' start', 0.1443752497434616), (' first', 0.007113280706107616), (' will', 0.005226947832852602)]]\n",
            "Step 41: Predicted Token:  to [Top 5: [(' to', 0.9361687302589417), (' more', 0.03480973094701767), (' the', 0.015948520973324776), (' additional', 0.00821960624307394), (' a', 0.0011212601093575358)]]\n",
            "Step 42: Predicted Token:  know [Top 5: [(' know', 0.31998175382614136), (' consider', 0.21773630380630493), (' use', 0.09954899549484253), (' follow', 0.09359614551067352), (' understand', 0.05083126947283745)]]\n",
            "Step 43: Predicted Token:  the [Top 5: [(' the', 0.879134476184845), (' that', 0.0368996299803257), (' two', 0.024720894172787666), (' which', 0.010029054246842861), (' how', 0.008978898636996746)]]\n",
            "Step 44: Predicted Token:  distance [Top 5: [(' distance', 0.41951313614845276), (' coordinates', 0.18370568752288818), (' total', 0.07895704358816147), (' exact', 0.03754684329032898), (' location', 0.029500549659132957)]]\n",
            "Step 45: Predicted Token:  from [Top 5: [(' from', 0.6827487349510193), (' between', 0.292911559343338), (' to', 0.009874118492007256), (' of', 0.002441872376948595), (' it', 0.002144656842574477)]]\n",
            "Step 46: Predicted Token:  Austin [Top 5: [(' Austin', 0.9553004503250122), (' the', 0.024976210668683052), (' one', 0.0028187825810164213), (' Texas', 0.002720736898481846), (' both', 0.0023275292478501797)]]\n",
            "Step 47: Predicted Token:  to [Top 5: [(' to', 0.791658341884613), (',', 0.2019091546535492), (' directly', 0.0020052308682352304), (' and', 0.0014884681440889835), (\"'s\", 0.0008492236956954002)]]\n",
            "Step 48: Predicted Token:  the [Top 5: [(' the', 0.9166513085365295), (' its', 0.03423347324132919), (' Austin', 0.012957477010786533), (' Texas', 0.006491079460829496), (' each', 0.0028247705195099115)]]\n",
            "Step 49: Predicted Token:  capital [Top 5: [(' capital', 0.6506615877151489), (' city', 0.06825599074363708), (' nearest', 0.049639154225587845), (' state', 0.04843371361494064), (' border', 0.030201418325304985)]]\n",
            "Step 50: Predicted Token:  of [Top 5: [(' of', 0.5904844403266907), ('.', 0.21914932131767273), (' and', 0.043468207120895386), (' in', 0.02649986557662487), (',', 0.02210412174463272)]]\n",
            "Step 51: Predicted Token:  Texas [Top 5: [(' Texas', 0.9911093711853027), (' the', 0.0045152814127504826), (' New', 0.0004566095012705773), (' Austin', 0.00031040661269798875), (' another', 0.0002879923558793962)]]\n",
            "Step 52: Predicted Token: . [Top 5: [('.', 0.6967440843582153), (',', 0.09992850571870804), (' and', 0.07408371567726135), (' itself', 0.02987566404044628), (' first', 0.028594089671969414)]]\n",
            "Step 53: Predicted Token:  According [Top 5: [(' According', 0.40210413932800293), (' However', 0.22995348274707794), (' The', 0.12549586594104767), (' Let', 0.10525507479906082), (' Since', 0.0607214979827404)]]\n",
            "Step 54: Predicted Token:  to [Top 5: [(' to', 0.999919056892395), (' the', 8.052063640207052e-05), (' you', 7.806593771420012e-08), ('o', 2.9760645503529304e-08), (' in', 2.7852154360630266e-08)]]\n",
            "Step 55: Predicted Token:  the [Top 5: [(' the', 0.9716286063194275), (' your', 0.011357116512954235), (' historical', 0.007959363050758839), (' standard', 0.002298231003805995), (' our', 0.0014056428335607052)]]\n",
            "Step 56: Predicted Token:  information [Top 5: [(' information', 0.6107254028320312), (' problem', 0.3527710735797882), (' given', 0.011374813504517078), (' question', 0.010855165310204029), (' data', 0.004127706866711378)]]\n",
            "Step 57: Predicted Token:  provided [Top 5: [(' provided', 0.8498744964599609), (' given', 0.14582297205924988), (',', 0.002457426628097892), (' you', 0.001128577976487577), (' in', 0.0003116120060440153)]]\n",
            "Step 58: Predicted Token: , [Top 5: [(',', 0.9328864216804504), (' in', 0.04118628427386284), (':\\n\\n', 0.025286778807640076), (':\\n', 0.00018572309636510909), (' (', 9.965739445760846e-05)]]\n",
            "Step 59: Predicted Token:  the [Top 5: [(' the', 0.7673174738883972), (' it', 0.16961412131786346), (' this', 0.02147769182920456), (' ', 0.020753858610987663), (' that', 0.003494640113785863)]]\n",
            "Step 60: Predicted Token:  distance [Top 5: [(' distance', 0.830350399017334), (' capital', 0.13903199136257172), (' answer', 0.003669623751193285), (' city', 0.003127650124952197), (' total', 0.0017655742121860385)]]\n",
            "Step 61: Predicted Token:  from [Top 5: [(' from', 0.8436681628227234), (' is', 0.1181163340806961), (' between', 0.032210931181907654), (' to', 0.0033506194595247507), (' of', 0.0014333475846797228)]]\n",
            "Step 62: Predicted Token:  Austin [Top 5: [(' Austin', 0.9965618252754211), (' the', 0.001112558995373547), (' Texas', 0.0006222514202818274), (' Houston', 0.0002468857856001705), (' both', 0.00012655688624363393)]]\n",
            "Step 63: Predicted Token:  to [Top 5: [(' to', 0.6620577573776245), (',', 0.3326597809791565), (' is', 0.003795020515099168), (' (', 0.0005971654318273067), (' and', 0.00043059492600150406)]]\n",
            "Step 64: Predicted Token:  the [Top 5: [(' the', 0.955288290977478), (' Austin', 0.03959782421588898), (' Houston', 0.0009998239111155272), (' Dallas', 0.0008695613360032439), (' Texas', 0.0008331436547450721)]]\n",
            "Step 65: Predicted Token:  capital [Top 5: [(' capital', 0.9927116632461548), (' cap', 0.002156005473807454), (' city', 0.00106140470597893), (' Capital', 0.000942841696087271), (' Texas', 0.0008175676339305937)]]\n",
            "Step 66: Predicted Token:  of [Top 5: [(' of', 0.988234281539917), (' is', 0.011304441839456558), (' in', 0.0001005866433843039), (' (', 8.990186324808747e-05), (' Texas', 6.500558811239898e-05)]]\n",
            "Step 67: Predicted Token:  Texas [Top 5: [(' Texas', 0.9996603727340698), (' the', 0.00010282383300364017), (' Austin', 7.16860085958615e-05), (' Dallas', 3.572498098947108e-05), ('Texas', 2.0835213945247233e-05)]]\n",
            "Step 68: Predicted Token:  is [Top 5: [(' is', 0.9940850734710693), (' (', 0.0024216030724346638), (',', 0.0023722024634480476), (' in', 0.0005201782332733274), (' can', 7.146740244934335e-05)]]\n",
            "Step 69: Predicted Token:   [Top 5: [(' ', 0.9547874927520752), (' given', 0.031043190509080887), (' exactly', 0.0021848557516932487), (' stated', 0.0017177185509353876), (' \\\\(', 0.0015908768400549889)]]\n",
            "Step 70: Predicted Token: 1 [Top 5: [('1', 0.9974006414413452), ('5', 0.0007980965892784297), ('2', 0.0006853134254924953), ('3', 0.00029317635926418006), ('4', 0.00025287800235673785)]]\n",
            "Step 71: Predicted Token: 0 [Top 5: [('0', 0.9997153878211975), ('5', 0.00011094052024418488), ('1', 6.420646968763322e-05), ('2', 4.1169019823428243e-05), (',', 1.8762571926345117e-05)]]\n",
            "Step 72: Predicted Token: 0 [Top 5: [('0', 0.9999097585678101), (' miles', 4.8859117669053376e-05), ('5', 9.908361789712217e-06), ('4', 6.608126113860635e-06), ('1', 6.471951110142982e-06)]]\n",
            "Step 73: Predicted Token:  miles [Top 5: [(' miles', 0.9999632835388184), (' kilometers', 8.919912033888977e-06), (' units', 7.98441942606587e-06), ('.', 3.76377533939376e-06), ('0', 2.5584411105228355e-06)]]\n",
            "Step 74: Predicted Token: .\n",
            "\n",
            " [Top 5: [('.\\n\\n', 0.5540821552276611), ('.', 0.4439692795276642), (' (', 0.0008067773305810988), (',', 0.0006471655215136707), ('.\\n', 8.407252607867122e-05)]]\n",
            "Step 75: Predicted Token: Therefore [Top 5: [('Therefore', 0.7712875008583069), ('So', 0.07665865868330002), ('Since', 0.03322773799300194), ('Thus', 0.03228919953107834), ('The', 0.027837632223963737)]]\n",
            "Step 76: Predicted Token: , [Top 5: [(',', 0.9999964237213135), (',\\n', 9.421935942555137e-07), (':\\n', 8.903001571525238e-07), (',\\n\\n', 6.771212497369561e-07), (':\\n\\n', 6.496081255136232e-07)]]\n",
            "Step 77: Predicted Token:  the [Top 5: [(' the', 0.9528253674507141), (' if', 0.028920674696564674), (' it', 0.011971810832619667), (' from', 0.0021583656780421734), (' there', 0.0009799003601074219)]]\n",
            "Step 78: Predicted Token:  distance [Top 5: [(' distance', 0.8882586359977722), (' answer', 0.0963994637131691), (' total', 0.0070197852328419685), (' capital', 0.0019473060965538025), (' final', 0.0016623215051367879)]]\n",
            "Step 79: Predicted Token:  from [Top 5: [(' from', 0.988135576248169), (' between', 0.00990920327603817), (' is', 0.0012665116228163242), (' directly', 0.00019324588356539607), (' to', 9.487321221968159e-05)]]\n",
            "Step 80: Predicted Token:  Austin [Top 5: [(' Austin', 0.9991602897644043), (' the', 0.00016680604312568903), ('Austin', 0.0001467281545046717), (' Texas', 9.278268407797441e-05), (' Aust', 4.825558062293567e-05)]]\n",
            "Step 81: Predicted Token: , [Top 5: [(',', 0.9224250316619873), (' to', 0.0773695707321167), (' (', 0.00011781106877606362), (' and', 3.263461621827446e-05), (' directly', 1.9288114344817586e-05)]]\n",
            "Step 82: Predicted Token:  Texas [Top 5: [(' Texas', 0.9997872710227966), (' which', 6.43352759652771e-05), (' to', 3.3466007153037935e-05), (' Dallas', 2.9751943657174706e-05), (' the', 9.301997124566697e-06)]]\n",
            "Step 83: Predicted Token: , [Top 5: [(',', 0.9850902557373047), (' to', 0.01480735931545496), (' (', 9.098734153667465e-05), (' and', 1.716602696433256e-06), (' towards', 1.0792088005473488e-06)]]\n",
            "Step 84: Predicted Token:  to [Top 5: [(' to', 0.9997890591621399), (' directly', 0.00010116628254763782), (' is', 6.543994095409289e-05), (' (', 5.818425961479079e-06), (' and', 4.537916538538411e-06)]]\n",
            "Step 85: Predicted Token:  the [Top 5: [(' the', 0.9988152980804443), (' Austin', 0.0008963492582552135), (' its', 0.00014806845865678042), (' any', 2.2175421690917574e-05), (' a', 1.5521400200668722e-05)]]\n",
            "Step 86: Predicted Token:  capital [Top 5: [(' capital', 0.9986912608146667), (' cap', 0.0008587987977080047), (' city', 0.00013276870595291257), (' Capital', 8.855747000779957e-05), (' center', 1.971814526768867e-05)]]\n",
            "Step 87: Predicted Token:  of [Top 5: [(' of', 0.9998169541358948), (' is', 0.00011345237726345658), (' (', 2.4677676265127957e-05), (',', 1.4630887562816497e-05), (' in', 6.177726845635334e-06)]]\n",
            "Step 88: Predicted Token:  Texas [Top 5: [(' Texas', 0.9998087286949158), (' the', 9.898562711896375e-05), ('Texas', 1.9832235921057872e-05), (' Dallas', 1.724754474707879e-05), (' Austin', 1.291330590902362e-05)]]\n",
            "Step 89: Predicted Token:  is [Top 5: [(' is', 0.9940616488456726), (',', 0.004011973273009062), (' (', 0.001151026925072074), (' would', 0.00024721233057789505), (' can', 0.00017970112094189972)]]\n",
            "Step 90: Predicted Token:  \\ [Top 5: [(' \\\\', 0.8877984881401062), (' ', 0.09380200505256653), (' simply', 0.005169270560145378), (':\\n\\n', 0.004316591192036867), (' $\\\\', 0.0030520546715706587)]]\n",
            "Step 91: Predicted Token: boxed [Top 5: [('boxed', 0.4998251795768738), ('(\\\\', 0.4997107684612274), ('_\\\\', 0.00013675990339834243), (' \\\\', 4.900389103568159e-05), (' (\\\\', 4.187061858829111e-05)]]\n",
            "Step 92: Predicted Token: { [Top 5: [('{', 0.999968409538269), ('{\\\\', 1.7105319784604944e-05), (' {', 5.514947133633541e-06), ('{x', 1.641197059143451e-06), ('{-', 1.438558228983311e-06)]]\n",
            "Step 93: Predicted Token: 1 [Top 5: [('1', 0.9998018145561218), ('2', 5.967503966530785e-05), ('5', 4.417577292770147e-05), ('9', 2.0110241166548803e-05), ('0', 1.776612589310389e-05)]]\n",
            "Step 94: Predicted Token: 0 [Top 5: [('0', 0.9999617338180542), ('5', 1.608614911674522e-05), ('2', 8.511702617397532e-06), ('1', 2.5545166408846853e-06), ('8', 2.1302544155332725e-06)]]\n",
            "Step 95: Predicted Token: 0 [Top 5: [('0', 0.9999872446060181), ('}', 5.225211225479143e-06), ('}.', 2.7013222734240117e-06), ('<|endoftext|>', 1.4452933783104527e-06), ('1', 8.575730703341833e-07)]]\n",
            "Step 96: Predicted Token: } [Top 5: [('}', 0.9643400311470032), ('}.', 0.03421814739704132), (' \\\\', 0.0009103603661060333), ('}\\\\', 0.0004148320003878325), ('\\\\', 8.949392940849066e-05)]]\n",
            "Step 97: Predicted Token:  miles [Top 5: [(' miles', 0.9994438290596008), ('.\\n\\n', 0.00018958332657348365), ('<|endoftext|>', 0.00014242967881727964), (' units', 9.582926577422768e-05), (' kilometers', 5.970478378003463e-05)]]\n",
            "Step 98: Predicted Token: . [Top 5: [('.', 0.9842931628227234), ('.\\n\\n', 0.014496457763016224), ('.\\n', 0.0007788541843183339), (' (', 9.119488095166162e-05), (',', 7.59396716603078e-05)]]\n",
            "Step 99: Predicted Token: <|endoftext|> [Top 5: [('<|endoftext|>', 0.9996789693832397), (' This', 9.595571464160457e-05), (' If', 6.027923518558964e-05), (' The', 5.584794416790828e-05), (' However', 2.6789799449034035e-05)]]\n",
            "<eos> token generated, stopping.\n",
            "\n",
            "Generated Text: 100 miles from Austin, Texas. How far is it from Austin, Texas to the capital of Texas?\n",
            "To determine the distance from Austin, Texas, to the capital of Texas, we need to know the distance from Austin to the capital of Texas. According to the information provided, the distance from Austin to the capital of Texas is 100 miles.\n",
            "\n",
            "Therefore, the distance from Austin, Texas, to the capital of Texas is \\boxed{100} miles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Batching Light Version\n",
        "\n",
        "prompts = [\n",
        "    \"The capital of Texas is Dallas or Austin?\",\n",
        "    \"Where McAllen is located\",\n",
        "    \"What is a transformer in LLM?\"\n",
        "]\n",
        "\n",
        "inputs = tokenizer(prompts,return_tensors=\"pt\", padding=True)\n",
        "# Notice the idea of padding, we have a batch: [batch_size,max_seq_length]\n",
        "# inputs = tokenizer(prompts,return_tensors=\"pt\") this line is problematic # you can't stack them together\n",
        "# because of different length\n",
        "inputs[\"input_ids\"].shape # so we have a 3 prompts which is expected\n",
        "# but since we have a different length of number of tokens we are getting padding\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03LpIFOpoyG0",
        "outputId": "6e269be6-79af-4146-cb7f-74ffc3969554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kq2Fle8pPhW",
        "outputId": "abf11041-36c7-4f1a-e7f7-8faec6df08f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   785,   6722,    315,   8257,    374,  18542,    476,  19260,     30],\n",
              "        [  9064,   4483,  79877,    374,   7407, 151643, 151643, 151643, 151643],\n",
              "        [  3838,    374,    264,  42578,    304,    444,  10994,     30, 151643]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# so notice those zeros in attention mask, so basically,\n",
        "# with padding = True, we are saying align our inputs\n",
        "# in the way that we have a matrix, where longest tokenized prompt\n",
        "# defines it's"
      ],
      "metadata": {
        "id": "F-0aOsyqpQyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'right'\n"
      ],
      "metadata": {
        "id": "g6AgELnis468"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this brings a problem\n",
        "# inputs = tokenizer(prompts,return_tensors=\"pt\", padding=True)\n",
        "# with torch.no_grad():\n",
        "#     out = model.generate(**inputs)\n",
        "\n",
        "# out.shape()"
      ],
      "metadata": {
        "id": "iYdbKKhDtDeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Note:\n",
        "# by default this one is called right padding but you can change it to left one\n",
        "tokenizer.padding_side = 'left'  # Change the padding side\n",
        "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
        "inputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRLhABFyrHpa",
        "outputId": "85e25fcc-c3d1-4147-8cd7-4e55226ea355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   785,   6722,    315,   8257,    374,  18542,    476,  19260,     30],\n",
              "        [151643, 151643, 151643, 151643,   9064,   4483,  79877,    374,   7407],\n",
              "        [151643,   3838,    374,    264,  42578,    304,    444,  10994,     30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "        [0, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.padding_side = 'left'\n",
        "inputs = tokenizer(prompts,return_tensors=\"pt\", padding=True)\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs,max_new_tokens=40)\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEIvxJkarVep",
        "outputId": "e816f79b-1b3a-4c6e-9782-49140a96e113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how long each prompt is (after left padding)\n",
        "input_lengths = inputs[\"attention_mask\"].sum(dim=1)  # tensor([L0, L1, L2])\n",
        "print(\"input lengths:\", input_lengths.tolist())\n",
        "\n",
        "# how many new tokens each row actually got\n",
        "gen_lengths = out.size(1) - input_lengths\n",
        "print(\"generated lengths:\", gen_lengths.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J0cTTPVt5-G",
        "outputId": "efd6b5c7-4c04-486b-9d54-d8afb31aa5b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input lengths: [9, 5, 8]\n",
            "generated lengths: [40, 44, 41]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only the new text per row\n",
        "\n",
        "continuations = []\n",
        "for i in range(out.size(0)):\n",
        "    L = int(input_lengths[i])              # length of the i-th prompt\n",
        "    cont_ids = out[i, L:]                  # tokens generated after the prompt\n",
        "    cont_ids = cont_ids.tolist()           # ensure CPU list of ints\n",
        "    text = tokenizer.decode(cont_ids, skip_special_tokens=True).strip() # try to remove skip_special_tokens=True\n",
        "    continuations.append(text)\n",
        "    print(f\"Row {i} continuation: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbGiW9jlu59z",
        "outputId": "b8a6bad8-edc2-403d-cdca-7c964c0f6d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row 0 continuation: Dallas\n",
            "Row 1 continuation: McAllen is located, it is a city that is known for its rich history and diverse culture. The city is home to many historic sites, museums, and cultural events that showcase the city's unique heritage. McAllen\n",
            "Row 2 continuation: ? A transformer is a device that converts alternating current (AC) from one voltage level to another. It is a type of electrical transformer that uses a core made of silicon steel to reduce the magnetic field strength\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "continuations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqG8xijgvBIC",
        "outputId": "ad592fb7-82a1-4aae-f077-00276f67d291"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dallas',\n",
              " \"McAllen is located, it is a city that is known for its rich history and diverse culture. The city is home to many historic sites, museums, and cultural events that showcase the city's unique heritage. McAllen\",\n",
              " '? A transformer is a device that converts alternating current (AC) from one voltage level to another. It is a type of electrical transformer that uses a core made of silicon steel to reduce the magnetic field strength']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "diWc4JCKvEaR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}